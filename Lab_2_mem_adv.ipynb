{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e67189-32d7-4459-a7bd-06601d1c3ab5",
   "metadata": {},
   "source": [
    "\n",
    "## Introducción a los componentes fundamentales de Langmem: `MemoryStoreManager` y `MemoryManager`\n",
    "\n",
    "En el ecosistema de **Langmem**, la gestión de la memoria es un aspecto crucial para el funcionamiento eficiente y coherente de agentes inteligentes y sistemas conversacionales. Dos bloques fundamentales en esta arquitectura son:\n",
    "\n",
    "- **MemoryStoreManager**: Responsable de gestionar el almacenamiento físico de los recuerdos o memorias. Administra las distintas fuentes de almacenamiento, ya sea en memoria local, bases de datos, o sistemas de almacenamiento distribuidos.\n",
    "\n",
    "- **MemoryManager**: Actúa como el orquestador lógico de las memorias. Controla cómo se crean, actualizan, recuperan y eliminan las memorias en función del contexto y las necesidades del agente.\n",
    "\n",
    "En esta sección, exploraremos en detalle el rol y la interacción de estos dos componentes, entendiendo cómo permiten a Langmem construir agentes más adaptativos, contextuales y persistentes en su conocimiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b5138-76ff-46e9-a222-6aa26e3eecbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b3059-950b-4db0-9d13-8f6deb5032f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4bce4a7-b352-4d50-9ce3-70953fe2a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las clases y tipos necesarios de langgraph\n",
    "from langgraph.store.base import BaseStore, Item, Op, Result\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from typing import Any, Iterable, Literal, NamedTuple, Optional, Union, cast\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "import boto3\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.func import entrypoint\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langmem import create_memory_store_manager\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_aws import ChatBedrock\n",
    "from IPython.display import Image, display\n",
    "from langmem import create_memory_manager # \n",
    "from pydantic import BaseModel\n",
    "# Definimos una clase personalizada que extiende de BaseStore\n",
    "class CustomMemoryStore(BaseStore):\n",
    "    \"\"\"\n",
    "    CustomMemoryStore es una implementación personalizada de BaseStore que\n",
    "    actúa como adaptador para un almacenamiento externo (ext_store).\n",
    "    Permite operaciones de get, put y batch de forma síncrona y asíncrona.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ext_store):\n",
    "        \"\"\"\n",
    "        Inicializa el CustomMemoryStore con un almacén externo.\n",
    "\n",
    "        Args:\n",
    "            ext_store: Un objeto que implementa los métodos get, put y batch.\n",
    "        \"\"\"\n",
    "        self.store = ext_store\n",
    "\n",
    "    def get(self, namespace: tuple[str, ...], key: str) -> Optional[Item]:\n",
    "        \"\"\"\n",
    "        Recupera un ítem del almacén dado un namespace y una clave.\n",
    "\n",
    "        Args:\n",
    "            namespace (tuple): Un espacio de nombres como tupla de strings.\n",
    "            key (str): La clave del ítem a recuperar.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Item]: El ítem encontrado o None si no existe.\n",
    "        \"\"\"\n",
    "        return self.store.get(namespace, key)\n",
    "\n",
    "    def put(self, namespace: tuple[str, ...], key: str, value: dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Guarda un ítem en el almacén bajo un namespace y una clave.\n",
    "\n",
    "        Args:\n",
    "            namespace (tuple): El espacio de nombres donde guardar.\n",
    "            key (str): La clave bajo la cual guardar el ítem.\n",
    "            value (dict): El valor del ítem a guardar.\n",
    "        \"\"\"\n",
    "        print(f\"PUT::namespace={namespace}, key={key}, value={value}:\")\n",
    "        return self.store.put(namespace, key, value)\n",
    "\n",
    "    async def aget(self, namespace: tuple[str, ...], key: str) -> Optional[Item]:\n",
    "        \"\"\"\n",
    "        Recupera un ítem de manera asíncrona.\n",
    "\n",
    "        Nota: Aunque se define como async, internamente llama al método síncrono get.\n",
    "\n",
    "        Args:\n",
    "            namespace (tuple): Espacio de nombres.\n",
    "            key (str): Clave a recuperar.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Item]: El ítem encontrado o None.\n",
    "        \"\"\"\n",
    "        res = await self.get(namespace, key)  # Puede lanzar advertencias ya que get no es async\n",
    "        return res\n",
    "\n",
    "    async def aput(self, namespace: tuple[str, ...], key: str, value: dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Guarda un ítem de forma asíncrona.\n",
    "\n",
    "        Nota: Llama al método síncrono put ya que no existe una versión nativa async.\n",
    "\n",
    "        Args:\n",
    "            namespace (tuple): Espacio de nombres.\n",
    "            key (str): Clave donde guardar.\n",
    "            value (dict): El valor a guardar.\n",
    "        \"\"\"\n",
    "        res = self.put(namespace, key, value)  # No se puede await porque put es síncrono\n",
    "        print(f\"ASYN-PUT::::namespace={namespace}, key={key}, value={value}:\")\n",
    "        return None\n",
    "\n",
    "    def batch(self, ops: Iterable[Op]) -> list[Result]:\n",
    "        \"\"\"\n",
    "        Ejecuta una serie de operaciones en lote de forma síncrona.\n",
    "\n",
    "        Args:\n",
    "            ops (Iterable[Op]): Lista de operaciones a realizar.\n",
    "\n",
    "        Returns:\n",
    "            list[Result]: Resultados de las operaciones.\n",
    "        \"\"\"\n",
    "        return self.store.batch(ops)\n",
    "\n",
    "    async def abatch(self, ops: Iterable[Op]) -> list[Result]:\n",
    "        \"\"\"\n",
    "        Ejecuta una serie de operaciones en lote de manera asíncrona.\n",
    "\n",
    "        Args:\n",
    "            ops (Iterable[Op]): Lista de operaciones a realizar.\n",
    "\n",
    "        Returns:\n",
    "            list[Result]: Resultados de las operaciones.\n",
    "        \"\"\"\n",
    "        print(f\"ASYNC::BATCH::::ops={ops}:\")\n",
    "        res = await self.store.abatch(ops)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2443d696-fd02-4269-a038-28e5fb4d4236",
   "metadata": {},
   "source": [
    "- Buena práctica: Podrías considerar envolver self.get en aget usando asyncio.to_thread para evitar advertencias de tipo (aunque depende del rendimiento que quieras optimizar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c818247-76c3-4d41-b6d6-5f23ffb88cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triple(BaseModel):\n",
    "    \"\"\"\n",
    "    Representa una estructura de triple (sujeto, predicado, objeto) utilizada para almacenar hechos, \n",
    "    preferencias y relaciones como triples en una base de datos o sistema de almacenamiento.\n",
    "    Un triple tiene un contexto opcional que ayuda a clasificar la información.\n",
    "\n",
    "    Atributos:\n",
    "    - subject: str\n",
    "      El sujeto del triple (por ejemplo, una entidad o persona).\n",
    "    - predicate: str\n",
    "      El predicado que describe la relación entre el sujeto y el objeto (por ejemplo, \"gestiona\", \"es miembro de\").\n",
    "    - object: str\n",
    "      El objeto que está relacionado con el sujeto (por ejemplo, otra persona, cosa o concepto).\n",
    "    - context: str | None\n",
    "      Un contexto opcional que puede usarse para proporcionar más detalles sobre el triple. Por defecto es None.\n",
    "    \"\"\"\n",
    "    subject: str  # Sujeto del triple\n",
    "    predicate: str  # Relación o acción\n",
    "    object: str  # Objeto del triple\n",
    "    context: str | None = None  # Contexto opcional, por defecto es None\n",
    "\n",
    "\n",
    "# Establecer el almacenamiento en memoria\n",
    "in_memory_store = CustomMemoryStore(InMemoryStore())\n",
    "# ---- ⚠️ Actualiza la región para tu configuración de AWS ⚠️\n",
    "region = 'us-east-2'  # Región de AWS para el cliente de Bedrock\n",
    "\n",
    "# Crear un cliente de AWS para el servicio de Bedrock\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,  # Usamos la región de AWS previamente configurada\n",
    ")\n",
    "\n",
    "# Crear el cliente específico para la API de Bedrock\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "\n",
    "# Definir el modelo que se va a usar (en este caso, un modelo de lenguaje de Anthropic)\n",
    "model_id = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "\n",
    "# Crear un objeto ChatBedrockConverse que se usará para invocar el modelo de lenguaje\n",
    "llm = ChatBedrockConverse(\n",
    "    model=model_id,  # Identificador del modelo\n",
    "    temperature=0,  # Controla la creatividad del modelo (0 significa determinístico)\n",
    "    max_tokens=5000,  # Número máximo de tokens a generar\n",
    "    client=bedrock_client,  # Cliente de AWS para hacer las llamadas\n",
    ")\n",
    "\n",
    "# Configurar un MemoryManager para gestionar el almacenamiento de triples (hechos y relaciones)\n",
    "memory_manager = create_memory_store_manager(\n",
    "    llm,  # El modelo de lenguaje que usará el manager\n",
    "    namespace=(\"chat\", \"{user_id}\", \"triples\"),  # Espacio de nombres único para cada usuario\n",
    "    schemas=[Triple],  # Usamos el esquema de Triple definido anteriormente\n",
    "    instructions=\"Extract all user information and events as triples.\",  # Instrucciones para extraer la información como triples\n",
    "    enable_inserts=True,  # Permitir insertar nuevos triples en la memoria\n",
    "    enable_deletes=True,  # Permitir eliminar triples de la memoria\n",
    ")\n",
    "\n",
    "# Ejemplo de un mensaje de usuario para ser procesado por el modelo\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",  # Especificamos que es un mensaje del usuario\n",
    "        \"content\": \"Alice manages the ML team and mentors Bob, who is also on the team.\",  # Contenido del mensaje\n",
    "    },\n",
    "]\n",
    "\n",
    "# Se comenta temporalmente esta parte porque se necesita un contexto adecuado para que funcione correctamente.\n",
    "# El código fallará si se invoca fuera de un contexto ejecutable de tipo \"RunnableContext\".\n",
    "# memory_manager.invoke({\"messages\": messages})\n",
    "\n",
    "# Definir la aplicación con el contexto de almacenamiento en memoria\n",
    "@entrypoint(store=in_memory_store)  # El decorador `entrypoint` permite que la función `app` reciba el almacenamiento\n",
    "def app(messages: list):\n",
    "    \"\"\"\n",
    "    Función principal de la aplicación que invoca el modelo de lenguaje y procesa los mensajes del usuario.\n",
    "\n",
    "    Parámetros:\n",
    "    - messages: list\n",
    "      Lista de mensajes que se pasan al modelo para obtener una respuesta.\n",
    "\n",
    "    Retorna:\n",
    "    - La respuesta del modelo de lenguaje basado en los mensajes proporcionados.\n",
    "    \"\"\"\n",
    "    # Invocar el modelo de lenguaje (ChatGPT o similar) con los mensajes\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",  # Mensaje del sistema que da instrucciones al modelo\n",
    "                \"content\": \"You are a helpful assistant.\",  # El rol del modelo como asistente\n",
    "            },\n",
    "            *messages  # Mensajes adicionales del usuario (expansión del mensaje)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extraer y almacenar los triples (hechos y relaciones) usando el MemoryManager\n",
    "    memory_manager.invoke({\"messages\": messages})\n",
    "\n",
    "    return response  # Devolver la respuesta generada por el modelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05bd2ac3-cca4-4088-be77-4329bba94f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUT::namespace=('chat', 'user123', 'triples'), key=4107f7c4-87ab-46b9-a487-3700a69d06e7, value={'kind': 'Triple', 'content': {'subject': 'Alice', 'predicate': 'manages', 'object': 'ML team', 'context': None}}:\n",
      "PUT::namespace=('chat', 'user123', 'triples'), key=e26a6ed7-275d-4a88-8d0a-8a6cb1742efa, value={'kind': 'Triple', 'content': {'subject': 'Alice', 'predicate': 'mentors', 'object': 'Bob', 'context': None}}:\n",
      "PUT::namespace=('chat', 'user123', 'triples'), key=98ad3d1d-e2a3-4702-ab70-2afbc59d3481, value={'kind': 'Triple', 'content': {'subject': 'Bob', 'predicate': 'is member of', 'object': 'ML team', 'context': None}}:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Okay, I understand. Alice is Bob's manager and mentor in the machine learning team.\", additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': 'ca82ef66-e970-4ba6-963e-0b895ae0fd73', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 29 Apr 2025 02:13:16 GMT', 'content-type': 'application/json', 'content-length': '265', 'connection': 'keep-alive', 'x-amzn-requestid': 'ca82ef66-e970-4ba6-963e-0b895ae0fd73'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [1007]}, 'model_name': 'us.anthropic.claude-3-5-haiku-20241022-v1:0'}, id='run-cb445bec-4577-4f43-85df-d356d43a0539-0', usage_metadata={'input_tokens': 30, 'output_tokens': 22, 'total_tokens': 52, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invocar la aplicación con un conjunto de mensajes y un contexto configurable que incluye un `user_id`\n",
    "app.invoke(messages, config={\"configurable\": {\"user_id\": \"user123\"}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bbbdab-1b47-4c6a-8cc7-05196e520cf6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🧠 Explicación del código (caso de uso)\n",
    "\n",
    "### **¿Qué caso de uso cubre este código?**\n",
    "\n",
    "Este código simula un asistente que **escucha lo que el usuario dice** y **extrae hechos importantes** de esa conversación para guardarlos en memoria.  \n",
    "**No utiliza** esos hechos guardados aún para responder. **Solo extrae y guarda**, nada más.\n",
    "\n",
    "**Ejemplo real**:  \n",
    "Un usuario dice \"Alice gestiona el equipo de ML\".  \n",
    "➡ El asistente **responde** normalmente al usuario (con su LLM).  \n",
    "➡ Y **aparte**, **extrae** el hecho \"**Alice - gestiona - equipo de ML**\" y lo **guarda** en memoria, por si algún día se quiere usar.\n",
    "\n",
    "---\n",
    "\n",
    "### **¿Qué hace el código exactamente?**\n",
    "\n",
    "1. **Instancia un modelo de lenguaje (LLM)** para que pueda responder preguntas (usa Bedrock y un modelo tipo Claude).\n",
    "   \n",
    "2. **Crea un gestor de memoria (Memory Manager)**, que se conecta a una base de memoria (aquí en memoria RAM, `InMemoryStore`).\n",
    "\n",
    "3. **Define qué se va a extraer**:\n",
    "   - En este caso, se van a extraer **triples** (sujeto, predicado, objeto).\n",
    "   - El Memory Manager ya tiene un **prompt especial** que le dice al modelo **qué debe extraer** (factos, relaciones, eventos...).\n",
    "\n",
    "4. **Cuando llega un mensaje**:\n",
    "   - **Primero** se manda el mensaje al modelo para que **genere una respuesta** al usuario (**primer `invoke`**).\n",
    "   - **Luego** se vuelve a mandar el mismo mensaje al Memory Manager, que **usa otra vez el modelo** para **extraer los hechos** y **guardarlos** (**segundo `invoke`**).\n",
    "\n",
    "---\n",
    "\n",
    "### **¿Qué **NO** hace este código?**\n",
    "\n",
    "- **No usa** todavía la memoria para mejorar las respuestas.\n",
    "- **No busca** en los datos guardados para responder.\n",
    "- **No hace reasoning** (razonamiento sobre lo que guardó).\n",
    "- Solo **extrae** y **guarda** datos, como si fueran apuntes rápidos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0303e092-cc44-4270-8836-a5fc59ba7000",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Crear un gestor de memoria que no utilice el almacenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae1cf4-0417-440e-ac70-6a3937f49402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mi_entorno_311)",
   "language": "python",
   "name": "mi_entorno_311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
